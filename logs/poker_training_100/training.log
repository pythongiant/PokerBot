2026-01-09 10:40:52,923 - Starting training with config:
{'name': 'poker_training_100', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 64, 'num_heads': 4, 'num_layers': 3, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 100, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-09 10:40:52,923 - Iteration 0: Running self-play...
2026-01-09 10:41:04,072 -   Average reward (P0): 0.656
2026-01-09 10:41:04,072 -   Training model...
2026-01-09 10:41:13,169 -   Losses - Policy: 0.0335, Value: 0.0060, Transition: 0.4596
2026-01-09 10:41:13,176 - Saved checkpoint to logs/poker_training_100/checkpoint_iter0.pt
2026-01-09 10:41:13,177 - Iteration 1: Running self-play...
2026-01-09 10:41:23,914 -   Average reward (P0): 0.602
2026-01-09 10:41:23,915 -   Training model...
2026-01-09 10:41:24,149 -   Losses - Policy: 0.0310, Value: 0.0016, Transition: 0.3583
2026-01-09 10:41:24,149 - Iteration 2: Running self-play...
2026-01-09 10:41:35,517 -   Average reward (P0): 0.539
2026-01-09 10:41:35,518 -   Training model...
2026-01-09 10:41:35,766 -   Losses - Policy: 0.0310, Value: 0.0012, Transition: 0.2525
2026-01-09 10:41:35,767 - Iteration 3: Running self-play...
2026-01-09 10:41:47,353 -   Average reward (P0): 0.906
2026-01-09 10:41:47,353 -   Training model...
2026-01-09 10:41:47,599 -   Losses - Policy: 0.0292, Value: 0.0005, Transition: 0.1646
2026-01-09 10:41:47,600 - Iteration 4: Running self-play...
2026-01-09 10:41:59,325 -   Average reward (P0): 0.609
2026-01-09 10:41:59,326 -   Training model...
2026-01-09 10:41:59,579 -   Losses - Policy: 0.0301, Value: 0.0005, Transition: 0.1332
2026-01-09 10:41:59,579 - Iteration 5: Running self-play...
2026-01-09 10:42:11,321 -   Average reward (P0): 0.656
2026-01-09 10:42:11,322 -   Training model...
2026-01-09 10:42:20,538 -   Losses - Policy: 0.0318, Value: 0.0011, Transition: 0.1403
2026-01-09 10:42:20,538 - Iteration 6: Running self-play...
2026-01-09 10:42:32,262 -   Average reward (P0): 0.484
2026-01-09 10:42:32,263 -   Training model...
2026-01-09 10:42:32,569 -   Losses - Policy: 0.0289, Value: 0.0010, Transition: 0.1216
2026-01-09 10:42:32,569 - Iteration 7: Running self-play...
2026-01-09 10:42:44,211 -   Average reward (P0): 0.328
2026-01-09 10:42:44,211 -   Training model...
2026-01-09 10:42:44,491 -   Losses - Policy: 0.0309, Value: 0.0123, Transition: 0.0913
2026-01-09 10:42:44,491 - Iteration 8: Running self-play...
