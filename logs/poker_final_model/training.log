2026-01-09 13:04:59,256 - Starting training with config:
{'name': 'poker_final_model', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 128, 'num_heads': 8, 'num_layers': 4, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 50, 'games_per_iteration': 256, 'batch_size': 64, 'learning_rate': 0.0005, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 100, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-09 13:04:59,256 - Iteration 0: Running self-play...
2026-01-09 13:06:11,915 -   Average reward (P0): 0.371
2026-01-09 13:06:11,916 -   Training model...
2026-01-09 13:06:56,226 -   Exploitability: 0.166701
2026-01-09 13:06:56,226 -   Losses - Policy: 0.0189, Value: 0.0119, Transition: 0.5014
2026-01-09 13:06:56,253 - Saved checkpoint to logs/poker_final_model/checkpoint_iter0.pt
2026-01-09 13:06:56,254 - Iteration 1: Running self-play...
2026-01-09 13:08:10,915 -   Average reward (P0): 0.586
2026-01-09 13:08:10,916 -   Training model...
2026-01-09 13:08:12,066 -   Losses - Policy: 0.0161, Value: 0.0017, Transition: 0.4562
2026-01-09 13:08:12,066 - Iteration 2: Running self-play...
2026-01-09 13:09:27,974 -   Average reward (P0): 0.730
2026-01-09 13:09:27,980 -   Training model...
2026-01-09 13:09:30,534 -   Losses - Policy: 0.0153, Value: 0.0006, Transition: 0.3984
2026-01-09 13:09:30,534 - Iteration 3: Running self-play...
2026-01-09 13:10:48,293 -   Average reward (P0): -0.031
2026-01-09 13:10:48,294 -   Training model...
2026-01-09 13:10:49,863 -   Losses - Policy: 0.0151, Value: 0.0004, Transition: 0.3253
2026-01-09 13:10:49,863 - Iteration 4: Running self-play...
2026-01-09 13:12:12,649 -   Average reward (P0): -0.008
2026-01-09 13:12:12,651 -   Training model...
2026-01-09 13:12:13,814 -   Losses - Policy: 0.0152, Value: 0.0002, Transition: 0.2545
2026-01-09 13:12:13,814 - Iteration 5: Running self-play...
2026-01-09 13:13:31,960 -   Average reward (P0): 0.676
2026-01-09 13:13:31,962 -   Training model...
2026-01-09 13:14:22,266 -   Exploitability: 0.166701
2026-01-09 13:14:22,266 -   Losses - Policy: 0.0158, Value: 0.0002, Transition: 0.2043
2026-01-09 13:14:22,266 - Iteration 6: Running self-play...
