2026-01-09 10:38:49,030 - Starting training with config:
{'name': 'poker_training_good', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 64, 'num_heads': 4, 'num_layers': 3, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 1000, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-09 10:38:49,030 - Iteration 0: Running self-play...
2026-01-09 10:39:00,192 -   Average reward (P0): 0.648
2026-01-09 10:39:00,192 -   Training model...
2026-01-09 10:39:07,091 -   Losses - Policy: 0.0312, Value: 0.0085, Transition: 0.4736
2026-01-09 10:39:07,106 - Saved checkpoint to logs/poker_training_good/checkpoint_iter0.pt
2026-01-09 10:39:07,106 - Iteration 1: Running self-play...
2026-01-09 10:39:18,376 -   Average reward (P0): 0.922
2026-01-09 10:39:18,377 -   Training model...
2026-01-09 10:39:18,677 -   Losses - Policy: 0.0314, Value: 0.0025, Transition: 0.3830
2026-01-09 10:39:18,677 - Iteration 2: Running self-play...
2026-01-09 10:39:29,251 -   Average reward (P0): 0.594
2026-01-09 10:39:29,251 -   Training model...
2026-01-09 10:39:29,462 -   Losses - Policy: 0.0291, Value: 0.0009, Transition: 0.2879
2026-01-09 10:39:29,462 - Iteration 3: Running self-play...
2026-01-09 10:39:41,386 -   Average reward (P0): 0.508
2026-01-09 10:39:41,386 -   Training model...
2026-01-09 10:39:41,683 -   Losses - Policy: 0.0296, Value: 0.0005, Transition: 0.2228
2026-01-09 10:39:41,683 - Iteration 4: Running self-play...
2026-01-09 10:39:53,076 -   Average reward (P0): 0.617
2026-01-09 10:39:53,076 -   Training model...
2026-01-09 10:39:53,335 -   Losses - Policy: 0.0301, Value: 0.0008, Transition: 0.1997
2026-01-09 10:39:53,335 - Iteration 5: Running self-play...
2026-01-09 10:40:05,986 -   Average reward (P0): 0.711
2026-01-09 10:40:05,986 -   Training model...
2026-01-09 10:40:12,989 -   Losses - Policy: 0.0300, Value: 0.0003, Transition: 0.2010
2026-01-09 10:40:12,989 - Iteration 6: Running self-play...
2026-01-09 10:40:25,897 -   Average reward (P0): 0.570
2026-01-09 10:40:25,898 -   Training model...
2026-01-09 10:40:26,227 -   Losses - Policy: 0.0291, Value: 0.0009, Transition: 0.1873
2026-01-09 10:40:26,227 - Iteration 7: Running self-play...
2026-01-09 10:40:38,431 -   Average reward (P0): 0.492
2026-01-09 10:40:38,432 -   Training model...
2026-01-09 10:40:38,703 -   Losses - Policy: 0.0299, Value: 0.0012, Transition: 0.1486
2026-01-09 10:40:38,703 - Iteration 8: Running self-play...
