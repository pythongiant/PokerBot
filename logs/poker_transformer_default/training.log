2026-01-08 13:03:42,510 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 64, 'num_heads': 4, 'num_layers': 3, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 100, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:03:42,510 - Iteration 0: Running self-play...
2026-01-08 13:04:16,363 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 64, 'num_heads': 4, 'num_layers': 3, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 5, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:04:16,364 - Iteration 0: Running self-play...
2026-01-08 13:04:36,878 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 32, 'num_heads': 2, 'num_layers': 2, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 1, 'games_per_iteration': 128, 'batch_size': 16, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:04:36,879 - Iteration 0: Running self-play...
2026-01-08 13:04:48,341 -   Average reward (P0): 0.625
2026-01-08 13:04:48,342 -   Training model...
2026-01-08 13:05:07,753 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 32, 'num_heads': 2, 'num_layers': 2, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 1, 'games_per_iteration': 128, 'batch_size': 16, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:05:07,753 - Iteration 0: Running self-play...
2026-01-08 13:05:18,874 -   Average reward (P0): 0.609
2026-01-08 13:05:18,874 -   Training model...
2026-01-08 13:05:38,682 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 32, 'num_heads': 2, 'num_layers': 2, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 1, 'games_per_iteration': 128, 'batch_size': 16, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:05:38,682 - Iteration 0: Running self-play...
2026-01-08 13:05:52,340 -   Average reward (P0): 0.828
2026-01-08 13:05:52,340 -   Training model...
2026-01-08 13:05:52,497 -   Losses - Policy: 0.0331, Value: 0.0015, Transition: 0.0000
2026-01-08 13:05:52,504 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter0.pt
2026-01-08 13:05:52,504 - Training complete!
2026-01-08 13:05:52,504 - Saved metrics to logs/poker_transformer_default/metrics.json
2026-01-08 13:05:52,504 - Generating visualizations...
2026-01-08 13:07:27,285 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 64, 'num_heads': 4, 'num_layers': 3, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 10, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:07:27,285 - Iteration 0: Running self-play...
2026-01-08 13:07:38,947 -   Average reward (P0): 0.578
2026-01-08 13:07:38,947 -   Training model...
2026-01-08 13:07:39,081 -   Losses - Policy: 0.0318, Value: 0.0116, Transition: 0.0000
2026-01-08 13:07:39,088 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter0.pt
2026-01-08 13:07:39,088 - Iteration 1: Running self-play...
2026-01-08 13:07:49,998 -   Average reward (P0): 0.562
2026-01-08 13:07:49,999 -   Training model...
2026-01-08 13:07:50,093 -   Losses - Policy: 0.0303, Value: 0.0031, Transition: 0.0000
2026-01-08 13:07:50,093 - Iteration 2: Running self-play...
2026-01-08 13:08:01,640 -   Average reward (P0): 0.398
2026-01-08 13:08:01,641 -   Training model...
2026-01-08 13:08:01,742 -   Losses - Policy: 0.0328, Value: 0.0012, Transition: 0.0000
2026-01-08 13:08:01,742 - Iteration 3: Running self-play...
2026-01-08 13:08:12,540 -   Average reward (P0): 0.891
2026-01-08 13:08:12,540 -   Training model...
2026-01-08 13:08:12,637 -   Losses - Policy: 0.0306, Value: 0.0005, Transition: 0.0000
2026-01-08 13:08:12,637 - Iteration 4: Running self-play...
2026-01-08 13:08:24,301 -   Average reward (P0): 0.234
2026-01-08 13:08:24,301 -   Training model...
2026-01-08 13:08:24,420 -   Losses - Policy: 0.0311, Value: 0.0003, Transition: 0.0000
2026-01-08 13:08:24,420 - Iteration 5: Running self-play...
2026-01-08 13:08:36,145 -   Average reward (P0): 0.648
2026-01-08 13:08:36,146 -   Training model...
2026-01-08 13:08:36,263 -   Losses - Policy: 0.0307, Value: 0.0002, Transition: 0.0000
2026-01-08 13:08:36,263 - Iteration 6: Running self-play...
2026-01-08 13:08:48,946 -   Average reward (P0): 0.875
2026-01-08 13:08:48,947 -   Training model...
2026-01-08 13:08:49,086 -   Losses - Policy: 0.0311, Value: 0.0001, Transition: 0.0000
2026-01-08 13:08:49,086 - Iteration 7: Running self-play...
2026-01-08 13:09:02,636 -   Average reward (P0): 0.805
2026-01-08 13:09:02,637 -   Training model...
2026-01-08 13:09:02,775 -   Losses - Policy: 0.0305, Value: 0.0001, Transition: 0.0000
2026-01-08 13:09:02,776 - Iteration 8: Running self-play...
2026-01-08 13:09:13,997 -   Average reward (P0): 0.695
2026-01-08 13:09:13,998 -   Training model...
2026-01-08 13:09:14,097 -   Losses - Policy: 0.0304, Value: 0.0001, Transition: 0.0000
2026-01-08 13:09:14,097 - Iteration 9: Running self-play...
2026-01-08 13:09:26,187 -   Average reward (P0): 0.570
2026-01-08 13:09:26,188 -   Training model...
2026-01-08 13:09:26,302 -   Losses - Policy: 0.0319, Value: 0.0001, Transition: 0.0000
2026-01-08 13:09:26,302 - Training complete!
2026-01-08 13:09:26,304 - Saved metrics to logs/poker_transformer_default/metrics.json
2026-01-08 13:09:26,304 - Generating visualizations...
2026-01-08 13:13:45,925 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 128, 'num_heads': 8, 'num_layers': 4, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 200, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:13:45,925 - Iteration 0: Running self-play...
2026-01-08 13:13:58,254 -   Average reward (P0): 0.828
2026-01-08 13:13:58,254 -   Training model...
2026-01-08 13:13:58,489 -   Losses - Policy: 0.0347, Value: 0.0143, Transition: 0.0000
2026-01-08 13:13:58,500 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter0.pt
2026-01-08 13:13:58,500 - Iteration 1: Running self-play...
2026-01-08 13:14:15,045 -   Average reward (P0): 0.820
2026-01-08 13:14:15,046 -   Training model...
2026-01-08 13:14:15,423 -   Losses - Policy: 0.0331, Value: 0.0005, Transition: 0.0000
2026-01-08 13:14:15,423 - Iteration 2: Running self-play...
2026-01-08 13:14:37,206 -   Average reward (P0): 0.492
2026-01-08 13:14:37,207 -   Training model...
2026-01-08 13:14:37,620 -   Losses - Policy: 0.0334, Value: 0.0003, Transition: 0.0000
2026-01-08 13:14:37,620 - Iteration 3: Running self-play...
2026-01-08 13:14:57,491 -   Average reward (P0): 1.031
2026-01-08 13:14:57,493 -   Training model...
2026-01-08 13:14:57,827 -   Losses - Policy: 0.0321, Value: 0.0001, Transition: 0.0000
2026-01-08 13:14:57,827 - Iteration 4: Running self-play...
2026-01-08 13:15:21,570 -   Average reward (P0): 0.711
2026-01-08 13:15:21,571 -   Training model...
2026-01-08 13:15:22,006 -   Losses - Policy: 0.0312, Value: 0.0001, Transition: 0.0000
2026-01-08 13:15:22,007 - Iteration 5: Running self-play...
2026-01-08 13:15:43,998 -   Average reward (P0): 0.648
2026-01-08 13:15:43,999 -   Training model...
2026-01-08 13:15:44,380 -   Losses - Policy: 0.0324, Value: 0.0000, Transition: 0.0000
2026-01-08 13:15:44,381 - Iteration 6: Running self-play...
2026-01-08 13:16:06,791 -   Average reward (P0): 0.836
2026-01-08 13:16:06,792 -   Training model...
2026-01-08 13:16:07,193 -   Losses - Policy: 0.0323, Value: 0.0000, Transition: 0.0000
2026-01-08 13:16:07,193 - Iteration 7: Running self-play...
2026-01-08 13:16:28,257 -   Average reward (P0): 0.703
2026-01-08 13:16:28,258 -   Training model...
2026-01-08 13:16:28,592 -   Losses - Policy: 0.0303, Value: 0.0000, Transition: 0.0000
2026-01-08 13:16:28,592 - Iteration 8: Running self-play...
2026-01-08 13:16:50,015 -   Average reward (P0): 0.711
2026-01-08 13:16:50,016 -   Training model...
2026-01-08 13:16:50,393 -   Losses - Policy: 0.0323, Value: 0.0000, Transition: 0.0000
2026-01-08 13:16:50,393 - Iteration 9: Running self-play...
2026-01-08 13:17:03,781 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 32, 'num_heads': 2, 'num_layers': 2, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 1, 'games_per_iteration': 128, 'batch_size': 16, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:17:03,781 - Iteration 0: Running self-play...
2026-01-08 13:20:37,443 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 64, 'num_heads': 4, 'num_layers': 3, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 3, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:20:37,444 - Iteration 0: Running self-play...
2026-01-08 13:21:01,264 -   Average reward (P0): 0.336
2026-01-08 13:21:01,266 -   Training model...
2026-01-08 13:21:01,552 -   Losses - Policy: 0.0337, Value: 0.0063, Transition: 0.0000
2026-01-08 13:21:01,565 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter0.pt
2026-01-08 13:21:01,565 - Iteration 1: Running self-play...
2026-01-08 13:21:22,056 -   Average reward (P0): 0.688
2026-01-08 13:21:22,058 -   Training model...
2026-01-08 13:21:22,289 -   Losses - Policy: 0.0308, Value: 0.0007, Transition: 0.0000
2026-01-08 13:21:22,289 - Iteration 2: Running self-play...
2026-01-08 13:21:42,109 -   Average reward (P0): 0.742
2026-01-08 13:21:42,110 -   Training model...
2026-01-08 13:21:42,317 -   Losses - Policy: 0.0317, Value: 0.0002, Transition: 0.0000
2026-01-08 13:21:42,317 - Training complete!
2026-01-08 13:21:42,318 - Saved metrics to logs/poker_transformer_default/metrics.json
2026-01-08 13:21:42,318 - Generating visualizations...
2026-01-08 13:40:58,013 - Starting training with config:
{'name': 'poker_transformer_default', 'environment': {'game_type': 'kuhn', 'initial_stack': 100, 'ante': 1, 'max_raises': 4, 'seed': 42}, 'model': {'latent_dim': 64, 'num_heads': 4, 'num_layers': 3, 'ff_dim': 256, 'dropout': 0.1, 'card_embed_dim': 16, 'action_embed_dim': 16, 'bet_embed_dim': 16, 'max_sequence_length': 128, 'transition_type': 'deterministic', 'value_hidden_dim': 128, 'policy_hidden_dim': 128, 'predict_opponent_range': False}, 'training': {'num_iterations': 100, 'games_per_iteration': 128, 'batch_size': 32, 'learning_rate': 0.001, 'optimizer': 'adam', 'weight_decay': 1e-05, 'grad_clip': 1.0, 'search_type': 'mcts', 'num_simulations': 50, 'rollout_depth': 10, 'value_bootstrap_mix': 0.99, 'use_amp': False, 'checkpoint_freq': 10, 'early_stopping_patience': 50}, 'evaluation': {'eval_games': 100, 'compute_exploitability': False, 'probe_beliefs': True, 'eval_vs_random': True, 'eval_vs_checkpoint': True}}
2026-01-08 13:40:58,013 - Iteration 0: Running self-play...
2026-01-08 13:41:10,509 -   Average reward (P0): 0.609
2026-01-08 13:41:10,509 -   Training model...
2026-01-08 13:41:10,673 -   Losses - Policy: 0.0332, Value: 0.0130, Transition: 0.0000
2026-01-08 13:41:10,680 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter0.pt
2026-01-08 13:41:10,680 - Iteration 1: Running self-play...
2026-01-08 13:41:22,473 -   Average reward (P0): 1.070
2026-01-08 13:41:22,474 -   Training model...
2026-01-08 13:41:22,595 -   Losses - Policy: 0.0303, Value: 0.0025, Transition: 0.0000
2026-01-08 13:41:22,595 - Iteration 2: Running self-play...
2026-01-08 13:41:34,479 -   Average reward (P0): 0.461
2026-01-08 13:41:34,479 -   Training model...
2026-01-08 13:41:34,602 -   Losses - Policy: 0.0293, Value: 0.0011, Transition: 0.0000
2026-01-08 13:41:34,602 - Iteration 3: Running self-play...
2026-01-08 13:41:47,126 -   Average reward (P0): 0.883
2026-01-08 13:41:47,127 -   Training model...
2026-01-08 13:41:47,260 -   Losses - Policy: 0.0290, Value: 0.0004, Transition: 0.0000
2026-01-08 13:41:47,260 - Iteration 4: Running self-play...
2026-01-08 13:42:00,504 -   Average reward (P0): 0.844
2026-01-08 13:42:00,505 -   Training model...
2026-01-08 13:42:00,655 -   Losses - Policy: 0.0320, Value: 0.0003, Transition: 0.0000
2026-01-08 13:42:00,655 - Iteration 5: Running self-play...
2026-01-08 13:42:12,316 -   Average reward (P0): 0.828
2026-01-08 13:42:12,316 -   Training model...
2026-01-08 13:42:12,434 -   Losses - Policy: 0.0297, Value: 0.0002, Transition: 0.0000
2026-01-08 13:42:12,434 - Iteration 6: Running self-play...
2026-01-08 13:42:24,048 -   Average reward (P0): 0.633
2026-01-08 13:42:24,049 -   Training model...
2026-01-08 13:42:24,167 -   Losses - Policy: 0.0304, Value: 0.0001, Transition: 0.0000
2026-01-08 13:42:24,167 - Iteration 7: Running self-play...
2026-01-08 13:42:35,423 -   Average reward (P0): 0.430
2026-01-08 13:42:35,423 -   Training model...
2026-01-08 13:42:35,538 -   Losses - Policy: 0.0299, Value: 0.0001, Transition: 0.0000
2026-01-08 13:42:35,538 - Iteration 8: Running self-play...
2026-01-08 13:42:48,063 -   Average reward (P0): 0.680
2026-01-08 13:42:48,063 -   Training model...
2026-01-08 13:42:48,217 -   Losses - Policy: 0.0305, Value: 0.0001, Transition: 0.0000
2026-01-08 13:42:48,218 - Iteration 9: Running self-play...
2026-01-08 13:42:59,405 -   Average reward (P0): 0.633
2026-01-08 13:42:59,406 -   Training model...
2026-01-08 13:42:59,522 -   Losses - Policy: 0.0298, Value: 0.0000, Transition: 0.0000
2026-01-08 13:42:59,522 - Iteration 10: Running self-play...
2026-01-08 13:43:10,575 -   Average reward (P0): 0.516
2026-01-08 13:43:10,576 -   Training model...
2026-01-08 13:43:10,683 -   Losses - Policy: 0.0322, Value: 0.0000, Transition: 0.0000
2026-01-08 13:43:10,690 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter10.pt
2026-01-08 13:43:10,690 - Iteration 11: Running self-play...
2026-01-08 13:43:22,607 -   Average reward (P0): 0.883
2026-01-08 13:43:22,608 -   Training model...
2026-01-08 13:43:22,735 -   Losses - Policy: 0.0310, Value: 0.0000, Transition: 0.0000
2026-01-08 13:43:22,735 - Iteration 12: Running self-play...
2026-01-08 13:43:35,539 -   Average reward (P0): 0.984
2026-01-08 13:43:35,540 -   Training model...
2026-01-08 13:43:35,707 -   Losses - Policy: 0.0300, Value: 0.0000, Transition: 0.0000
2026-01-08 13:43:35,708 - Iteration 13: Running self-play...
2026-01-08 13:43:49,096 -   Average reward (P0): 0.766
2026-01-08 13:43:49,096 -   Training model...
2026-01-08 13:43:49,263 -   Losses - Policy: 0.0304, Value: 0.0000, Transition: 0.0000
2026-01-08 13:43:49,263 - Iteration 14: Running self-play...
2026-01-08 13:44:01,942 -   Average reward (P0): 0.812
2026-01-08 13:44:01,943 -   Training model...
2026-01-08 13:44:02,084 -   Losses - Policy: 0.0318, Value: 0.0000, Transition: 0.0000
2026-01-08 13:44:02,084 - Iteration 15: Running self-play...
2026-01-08 13:44:13,454 -   Average reward (P0): 0.664
2026-01-08 13:44:13,454 -   Training model...
2026-01-08 13:44:13,571 -   Losses - Policy: 0.0314, Value: 0.0000, Transition: 0.0000
2026-01-08 13:44:13,571 - Iteration 16: Running self-play...
2026-01-08 13:44:26,097 -   Average reward (P0): 0.672
2026-01-08 13:44:26,097 -   Training model...
2026-01-08 13:44:26,246 -   Losses - Policy: 0.0331, Value: 0.0000, Transition: 0.0000
2026-01-08 13:44:26,246 - Iteration 17: Running self-play...
2026-01-08 13:44:38,604 -   Average reward (P0): 0.656
2026-01-08 13:44:38,604 -   Training model...
2026-01-08 13:44:38,756 -   Losses - Policy: 0.0290, Value: 0.0000, Transition: 0.0000
2026-01-08 13:44:38,756 - Iteration 18: Running self-play...
2026-01-08 13:44:50,378 -   Average reward (P0): 0.469
2026-01-08 13:44:50,378 -   Training model...
2026-01-08 13:44:50,521 -   Losses - Policy: 0.0311, Value: 0.0000, Transition: 0.0000
2026-01-08 13:44:50,522 - Iteration 19: Running self-play...
2026-01-08 13:45:01,713 -   Average reward (P0): 0.695
2026-01-08 13:45:01,713 -   Training model...
2026-01-08 13:45:01,856 -   Losses - Policy: 0.0327, Value: 0.0000, Transition: 0.0000
2026-01-08 13:45:01,856 - Iteration 20: Running self-play...
2026-01-08 13:45:13,908 -   Average reward (P0): 0.648
2026-01-08 13:45:13,909 -   Training model...
2026-01-08 13:45:14,053 -   Losses - Policy: 0.0312, Value: 0.0000, Transition: 0.0000
2026-01-08 13:45:14,060 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter20.pt
2026-01-08 13:45:14,062 - Iteration 21: Running self-play...
2026-01-08 13:45:25,252 -   Average reward (P0): 0.812
2026-01-08 13:45:25,253 -   Training model...
2026-01-08 13:45:25,383 -   Losses - Policy: 0.0304, Value: 0.0000, Transition: 0.0000
2026-01-08 13:45:25,383 - Iteration 22: Running self-play...
2026-01-08 13:45:36,136 -   Average reward (P0): 0.648
2026-01-08 13:45:36,137 -   Training model...
2026-01-08 13:45:36,248 -   Losses - Policy: 0.0303, Value: 0.0000, Transition: 0.0000
2026-01-08 13:45:36,248 - Iteration 23: Running self-play...
2026-01-08 13:45:48,072 -   Average reward (P0): 0.500
2026-01-08 13:45:48,073 -   Training model...
2026-01-08 13:45:48,209 -   Losses - Policy: 0.0302, Value: 0.0000, Transition: 0.0000
2026-01-08 13:45:48,210 - Iteration 24: Running self-play...
2026-01-08 13:45:59,714 -   Average reward (P0): 0.156
2026-01-08 13:45:59,715 -   Training model...
2026-01-08 13:45:59,883 -   Losses - Policy: 0.0291, Value: 0.0000, Transition: 0.0000
2026-01-08 13:45:59,883 - Iteration 25: Running self-play...
2026-01-08 13:46:11,356 -   Average reward (P0): 0.750
2026-01-08 13:46:11,357 -   Training model...
2026-01-08 13:46:11,494 -   Losses - Policy: 0.0315, Value: 0.0000, Transition: 0.0000
2026-01-08 13:46:11,494 - Iteration 26: Running self-play...
2026-01-08 13:46:22,903 -   Average reward (P0): 0.734
2026-01-08 13:46:22,903 -   Training model...
2026-01-08 13:46:23,024 -   Losses - Policy: 0.0302, Value: 0.0000, Transition: 0.0000
2026-01-08 13:46:23,024 - Iteration 27: Running self-play...
2026-01-08 13:46:35,608 -   Average reward (P0): 0.484
2026-01-08 13:46:35,609 -   Training model...
2026-01-08 13:46:35,784 -   Losses - Policy: 0.0302, Value: 0.0000, Transition: 0.0000
2026-01-08 13:46:35,785 - Iteration 28: Running self-play...
2026-01-08 13:46:46,704 -   Average reward (P0): 0.492
2026-01-08 13:46:46,705 -   Training model...
2026-01-08 13:46:46,820 -   Losses - Policy: 0.0309, Value: 0.0000, Transition: 0.0000
2026-01-08 13:46:46,820 - Iteration 29: Running self-play...
2026-01-08 13:46:57,518 -   Average reward (P0): 0.375
2026-01-08 13:46:57,518 -   Training model...
2026-01-08 13:46:57,649 -   Losses - Policy: 0.0286, Value: 0.0000, Transition: 0.0000
2026-01-08 13:46:57,649 - Iteration 30: Running self-play...
2026-01-08 13:47:09,330 -   Average reward (P0): 0.977
2026-01-08 13:47:09,330 -   Training model...
2026-01-08 13:47:09,482 -   Losses - Policy: 0.0303, Value: 0.0000, Transition: 0.0000
2026-01-08 13:47:09,491 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter30.pt
2026-01-08 13:47:09,491 - Iteration 31: Running self-play...
2026-01-08 13:47:21,415 -   Average reward (P0): 0.617
2026-01-08 13:47:21,416 -   Training model...
2026-01-08 13:47:21,553 -   Losses - Policy: 0.0309, Value: 0.0000, Transition: 0.0000
2026-01-08 13:47:21,553 - Iteration 32: Running self-play...
2026-01-08 13:47:32,333 -   Average reward (P0): 0.641
2026-01-08 13:47:32,334 -   Training model...
2026-01-08 13:47:32,439 -   Losses - Policy: 0.0297, Value: 0.0000, Transition: 0.0000
2026-01-08 13:47:32,439 - Iteration 33: Running self-play...
2026-01-08 13:47:44,254 -   Average reward (P0): 0.570
2026-01-08 13:47:44,255 -   Training model...
2026-01-08 13:47:44,387 -   Losses - Policy: 0.0296, Value: 0.0000, Transition: 0.0000
2026-01-08 13:47:44,388 - Iteration 34: Running self-play...
2026-01-08 13:47:56,360 -   Average reward (P0): 0.633
2026-01-08 13:47:56,361 -   Training model...
2026-01-08 13:47:56,503 -   Losses - Policy: 0.0325, Value: 0.0000, Transition: 0.0000
2026-01-08 13:47:56,504 - Iteration 35: Running self-play...
2026-01-08 13:48:08,090 -   Average reward (P0): 0.914
2026-01-08 13:48:08,090 -   Training model...
2026-01-08 13:48:08,219 -   Losses - Policy: 0.0292, Value: 0.0000, Transition: 0.0000
2026-01-08 13:48:08,219 - Iteration 36: Running self-play...
2026-01-08 13:48:19,282 -   Average reward (P0): 1.109
2026-01-08 13:48:19,282 -   Training model...
2026-01-08 13:48:19,404 -   Losses - Policy: 0.0303, Value: 0.0000, Transition: 0.0000
2026-01-08 13:48:19,404 - Iteration 37: Running self-play...
2026-01-08 13:48:30,356 -   Average reward (P0): 0.719
2026-01-08 13:48:30,356 -   Training model...
2026-01-08 13:48:30,469 -   Losses - Policy: 0.0291, Value: 0.0000, Transition: 0.0000
2026-01-08 13:48:30,469 - Iteration 38: Running self-play...
2026-01-08 13:48:41,420 -   Average reward (P0): 0.625
2026-01-08 13:48:41,421 -   Training model...
2026-01-08 13:48:41,551 -   Losses - Policy: 0.0323, Value: 0.0000, Transition: 0.0000
2026-01-08 13:48:41,551 - Iteration 39: Running self-play...
2026-01-08 13:48:52,569 -   Average reward (P0): 0.414
2026-01-08 13:48:52,569 -   Training model...
2026-01-08 13:48:52,696 -   Losses - Policy: 0.0318, Value: 0.0000, Transition: 0.0000
2026-01-08 13:48:52,696 - Iteration 40: Running self-play...
2026-01-08 13:49:03,802 -   Average reward (P0): 0.898
2026-01-08 13:49:03,802 -   Training model...
2026-01-08 13:49:03,928 -   Losses - Policy: 0.0307, Value: 0.0000, Transition: 0.0000
2026-01-08 13:49:03,935 - Saved checkpoint to logs/poker_transformer_default/checkpoint_iter40.pt
2026-01-08 13:49:03,935 - Iteration 41: Running self-play...
2026-01-08 13:49:13,765 -   Average reward (P0): 0.188
2026-01-08 13:49:13,765 -   Training model...
2026-01-08 13:49:13,866 -   Losses - Policy: 0.0288, Value: 0.0000, Transition: 0.0000
2026-01-08 13:49:13,867 - Iteration 42: Running self-play...
